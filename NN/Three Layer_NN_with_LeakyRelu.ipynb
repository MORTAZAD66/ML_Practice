{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import gzip, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:     (50000, 784)\n",
      "Training labels shape:   (50000,)\n",
      "Validation data shape:   (10000, 784)\n",
      "Validation labels shape: (10000,)\n",
      "Test data shape:         (10000, 784)\n",
      "Test labels shape:       (10000,)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'Data/mnist.pkl.gz'\n",
    "\n",
    "with gzip.open(DATA_PATH, 'rb') as f:\n",
    "    (X_train, y_train), (X_valid, y_valid), (X_test, y_test) = pickle.load(f, encoding='latin')\n",
    "\n",
    "print('Training data shape:    ', X_train.shape)\n",
    "print('Training labels shape:  ', y_train.shape)\n",
    "print('Validation data shape:  ', X_valid.shape)\n",
    "print('Validation labels shape:', y_valid.shape)\n",
    "print('Test data shape:        ', X_test.shape)\n",
    "print('Test labels shape:      ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, b, X):\n",
    "    scores = X @ W + b\n",
    "    return np.argmax(scores, axis=1)\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return 100. * np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: training loss = 2.30 | validation loss = 2.30\n",
      "  2: training loss = 2.30 | validation loss = 2.30\n",
      "  3: training loss = 2.30 | validation loss = 2.30\n",
      "  4: training loss = 2.30 | validation loss = 2.30\n",
      "  5: training loss = 2.30 | validation loss = 2.30\n",
      "  6: training loss = 2.30 | validation loss = 2.30\n",
      "  7: training loss = 2.11 | validation loss = 1.52\n",
      "  8: training loss = 1.23 | validation loss = 0.80\n",
      "  9: training loss = 0.58 | validation loss = 0.38\n",
      " 10: training loss = 0.35 | validation loss = 0.25\n",
      " 11: training loss = 0.27 | validation loss = 0.20\n",
      " 12: training loss = 0.25 | validation loss = 0.18\n",
      " 13: training loss = 0.23 | validation loss = 0.17\n",
      " 14: training loss = 0.21 | validation loss = 0.16\n",
      " 15: training loss = 0.20 | validation loss = 0.15\n",
      " 16: training loss = 0.19 | validation loss = 0.14\n",
      " 17: training loss = 0.19 | validation loss = 0.14\n",
      " 18: training loss = 0.18 | validation loss = 0.13\n",
      " 19: training loss = 0.17 | validation loss = 0.13\n",
      " 20: training loss = 0.17 | validation loss = 0.13\n",
      " 21: training loss = 0.17 | validation loss = 0.12\n",
      " 22: training loss = 0.16 | validation loss = 0.11\n",
      " 23: training loss = 0.16 | validation loss = 0.12\n",
      " 24: training loss = 0.16 | validation loss = 0.11\n",
      " 25: training loss = 0.16 | validation loss = 0.11\n",
      " 26: training loss = 0.15 | validation loss = 0.11\n",
      " 27: training loss = 0.15 | validation loss = 0.12\n",
      " 28: training loss = 0.15 | validation loss = 0.10\n",
      " 29: training loss = 0.15 | validation loss = 0.11\n",
      " 30: training loss = 0.15 | validation loss = 0.10\n"
     ]
    }
   ],
   "source": [
    "# Leaky ReLU activation function\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.maximum(alpha*x, x)\n",
    "\n",
    "# Derivative of Leaky ReLU activation function\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    dx = np.ones_like(x)\n",
    "    dx[x < 0] = alpha\n",
    "    return dx\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=1, keepdims=True) \n",
    "    ### This subtraction of the maximum value helps prevent numerical overflow or instability that can occur when taking the exponential of large numbers.\n",
    "    ### It is a common practice to improve the numerical stability of the softmax function, \n",
    "    ## which is often used in the output layer of neural networks for multi-class classification problems.\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss_2(scores, y, mode='train'):\n",
    "    m = scores.shape[0] #number of data\n",
    "    probs = softmax(scores)\n",
    "    loss = -np.sum(np.log(probs[range(m), y])) / m\n",
    "    \n",
    "    if mode != 'train':\n",
    "        return loss\n",
    "    \n",
    "    # backward\n",
    "    dscores = probs\n",
    "    dscores[range(m), y] -= 1.0\n",
    "    dscores /= m\n",
    "    \n",
    "    return loss, dscores\n",
    "\n",
    "def softmax_loss_3(scores, y, mode='train'):\n",
    "    m = scores.shape[0] #number of data\n",
    "    probs = softmax(scores)\n",
    "    loss = -np.sum(np.log(probs[range(m), y])) / m\n",
    "    \n",
    "    if mode != 'train':\n",
    "        return loss\n",
    "    \n",
    "    # backward\n",
    "    dscores = probs\n",
    "    dscores[range(m), y] -= 1.0\n",
    "    dscores /= m\n",
    "    \n",
    "    return loss, dscores\n",
    "\n",
    "class ThreeLayerNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_features=784, num_hiddens=20, num_classes=10):\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # random initialization: create random weights, set all biases to zero\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(num_features, num_hiddens) * 0.001\n",
    "        self.params['W2'] = np.random.randn(num_hiddens, num_hiddens) * 0.001\n",
    "        self.params['W3'] = np.random.randn(num_hiddens,  num_classes) * 0.001\n",
    "        self.params['b1'] = np.zeros((num_hiddens,))\n",
    "        self.params['b2'] = np.zeros((num_hiddens,))\n",
    "        self.params['b3'] = np.zeros((num_classes,))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # forward step\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "        # forward step\n",
    "        h_in_1 = X @ W1 + b1       # hidden layer1 input\n",
    "        #h_1 = np.maximum(0, h_in_1)  # hidden layer1 output (using ReLU)\n",
    "        h_1 = leaky_relu(h_in_1)        #Using Leaky Relu\n",
    "        h_in_2 = h_1 @ W2 + b2       # hidden layer2 input\n",
    "        #h_2 = np.maximum(0, h_in_2)  # hidden layer2 output (using ReLU)\n",
    "        h_2 = leaky_relu(h_in_2)        #Using Leaky Relu\n",
    "        scores = h_2 @ W3 + b3     # neural net output\n",
    "        \n",
    "        return scores\n",
    "                            \n",
    "    def train_step(self, X, y):\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        W3, b3 = self.params['W3'], self.params['b3']\n",
    "        \n",
    "        # forward step\n",
    "        z_1 = X @ W1 + b1       # hidden layer1 input\n",
    "        #a_1 = np.maximum(0, z_1)  # hidden layer1 output (using ReLU)\n",
    "        a_1 = leaky_relu(z_1)       #Using Leaky Relu\n",
    "        z_2 = a_1 @ W2 + b2       # hidden layer2 input\n",
    "        #a_2 = np.maximum(0, z_2)  # hidden layer2 output (using ReLU)\n",
    "        a_2 = leaky_relu(z_2)       #Using Leaky Relu\n",
    "        scores = a_2 @ W3 + b3     # neural net output\n",
    "        \n",
    "        # compute loss\n",
    "        loss, dscores = softmax_loss_3(scores, y)\n",
    "        \n",
    "        # backward step\n",
    "        db3 = dscores.sum(axis=0)\n",
    "        dW3 = a_2.T @ dscores\n",
    "\n",
    "        da_2 = dscores @ W3.T\n",
    "        #da_2[a_2 < 0] = 0.0  # ReLU derivative for second hidden layer\n",
    "        dz_2 = da_2*leaky_relu_derivative(z_2)      #Using Leaky Relu\n",
    "\n",
    "        db2 = dz_2.sum(axis=0)\n",
    "        dW2 = a_1.T @ dz_2\n",
    "\n",
    "        da_1 = dz_2 @ W2.T\n",
    "        #da_1[a_1 < 0] = 0.0  # ReLU derivative for first hidden layer\n",
    "        dz_1 = da_1*leaky_relu_derivative(z_1)      #Using Leaky Relu\n",
    "\n",
    "        db1 = dz_1.sum(axis=0)\n",
    "        dW1 = X.T @ dz_1\n",
    "\n",
    "        gradient = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2, 'W3': dW3, 'b3': db3}\n",
    "\n",
    "        return loss, gradient\n",
    "\n",
    "        \n",
    "    def train(self, X_train, y_train, X_valid, y_valid, batch_size=50, \n",
    "              alpha=0.001, lmbda=0.0001, num_epochs=20):\n",
    "        \n",
    "        m, n = X_train.shape        \n",
    "        num_batches = m // batch_size\n",
    "        \n",
    "        report = \"{:3d}: training loss = {:.2f} | validation loss = {:.2f}\"\n",
    "        \n",
    "        losses = []\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            \n",
    "            for _ in range(num_batches):\n",
    "                W1, b1 = self.params['W1'], self.params['b1']\n",
    "                W2, b2 = self.params['W2'], self.params['b2']\n",
    "                W3, b3 = self.params['W3'], self.params['b3']\n",
    "                \n",
    "                # select a random mini-batch\n",
    "                batch_idx = np.random.choice(m, batch_size, replace=False)\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "\n",
    "                # train on mini-batch\n",
    "                data_loss, gradient = self.train_step(X_batch, y_batch)\n",
    "                reg_loss = 0.5 * (np.sum(W1 ** 2) + np.sum(W2 ** 2) + np.sum(W3 ** 2))\n",
    "                train_loss += (data_loss + lmbda * reg_loss)\n",
    "                losses.append(data_loss + lmbda * reg_loss)\n",
    "\n",
    "                # regularization\n",
    "                gradient['W1'] += lmbda * W1\n",
    "                gradient['W2'] += lmbda * W2\n",
    "                gradient['W3'] += lmbda * W3\n",
    "\n",
    "                # update parameters\n",
    "                for p in self.params:\n",
    "                    self.params[p] = self.params[p] - alpha * gradient[p]\n",
    "            \n",
    "            # report training loss and validation loss\n",
    "            train_loss /= num_batches\n",
    "            valid_loss = softmax_loss_2(self.forward(X_valid), y_valid, mode='test')\n",
    "            print(report.format(epoch + 1, train_loss, valid_loss))\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict probabilties of classes for each input data.\n",
    "        \"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return softmax(scores)\n",
    "\n",
    "mlp3 = ThreeLayerNeuralNetwork(num_hiddens=40)\n",
    "losses = mlp3.train(X_train, y_train, X_valid, y_valid, \n",
    "                   alpha=0.08, lmbda=0.001, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy   = 98.16%\n",
      "Test accuracy = 97.02%\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(mlp3.predict(X_train), y_train)\n",
    "print(\"Train accuracy   = {:.2f}%\".format(train_acc))\n",
    "\n",
    "test_acc = accuracy(mlp3.predict(X_test), y_test)\n",
    "print(\"Test accuracy = {:.2f}%\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
